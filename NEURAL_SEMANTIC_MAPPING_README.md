# PONI Neural Semantic Mapping Scripts

This directory contains scripts that demonstrate how to use PONI's neural `Semantic_Mapping` class to generate predicted semantic maps from RGB-D observations and save them to files.

## Overview

The neural `Semantic_Mapping` class is different from the ground truth semantic map creation in `create_semantic_maps.py`. While `create_semantic_maps.py` processes complete 3D scenes offline to create ground truth maps, the neural `Semantic_Mapping` class operates online during navigation, incrementally building semantic maps from partial RGB-D observations.

### Key Differences:
- **Ground Truth (`create_semantic_maps.py`)**: Offline processing of complete 3D scenes (GLB, PLY files) → 2D semantic maps
- **Neural Prediction (`Semantic_Mapping` class)**: Online processing of RGB-D observations → Incremental semantic map predictions

## Scripts

### 1. `create_predicted_semantic_maps.py`

Demonstrates how to use PONI's neural `Semantic_Mapping` class to generate predicted semantic maps from RGB-D observations.

**Features:**
- Initializes the neural semantic mapping module
- Simulates RGB-D observations and pose changes
- Incrementally builds semantic maps using the neural network
- Saves predicted maps in HDF5 format
- Creates visualizations of the mapping process

**Usage:**
```bash
# Basic usage
python create_predicted_semantic_maps.py --num_steps 10 --save_dir ./predicted_maps

# With visualizations
python create_predicted_semantic_maps.py --num_steps 20 --save_dir ./predicted_maps --visualize

# On CPU (if no GPU available)
python create_predicted_semantic_maps.py --device cpu --num_steps 5 --save_dir ./predicted_maps
```

**Arguments:**
- `--num_steps`: Number of simulation steps (default: 10)
- `--save_dir`: Directory to save predicted maps (default: ./predicted_maps)
- `--device`: Device to run on (default: cuda:0)
- `--visualize`: Save visualizations of semantic maps
- `--save_raw`: Save raw semantic maps as HDF5 (default: True)

### 2. `visualize_predicted_maps.py`

Loads and visualizes the predicted semantic maps generated by the first script.

**Features:**
- Loads semantic maps from HDF5 files
- Visualizes different map channels (obstacle, explored, semantic categories)
- Analyzes semantic category predictions
- Creates animations showing map evolution over time
- Generates comprehensive visualizations and statistics

**Usage:**
```bash
# Visualize a specific map file
python visualize_predicted_maps.py --map_file ./predicted_maps/final_semantic_map.h5

# Visualize all maps in a directory and create animation
python visualize_predicted_maps.py --map_dir ./predicted_maps --create_animation

# Save visualizations to specific directory
python visualize_predicted_maps.py --map_file ./predicted_maps/final_semantic_map.h5 --save_dir ./my_visualizations
```

**Arguments:**
- `--map_file`: Path to specific map file to visualize
- `--map_dir`: Directory containing map files
- `--save_dir`: Directory to save visualizations (default: ./visualizations)
- `--create_animation`: Create animation from sequential maps
- `--analyze_categories`: Analyze semantic categories (default: True)

## Output Files

### Generated by `create_predicted_semantic_maps.py`:

1. **Sequential Maps**: `semantic_map_step_XXX.h5`
   - Individual semantic maps for each simulation step
   - Contains step metadata and pose information

2. **Final Map**: `final_semantic_map.h5`
   - Complete accumulated semantic map
   - Contains all map channels:
     - `full_map`: All channels combined
     - `semantic_map`: Semantic category predictions only
     - `obstacle_map`: Obstacle predictions
     - `explored_map`: Explored area map
     - `agent_map`: Current agent location
     - `past_locations_map`: Agent trajectory

3. **Visualizations** (if `--visualize` enabled):
   - `semantic_map_vis_step_XXX.png`: Individual step visualizations
   - `final_semantic_map_vis.png`: Final map visualization

4. **Summary**: `experiment_summary.json`
   - Experiment parameters and statistics
   - Final pose and map coverage information

### Generated by `visualize_predicted_maps.py`:

1. **Comprehensive Visualization**: `map_channels_visualization.png`
   - 6-panel visualization showing all map channels
   - Navigation map with obstacles, explored areas, and trajectory

2. **Category Analysis**: `top_semantic_categories.png`
   - Visualization of top predicted semantic categories
   - Individual category maps with statistics

3. **Animation**: `semantic_mapping_evolution.gif`
   - Time-lapse showing map evolution over simulation steps

## Data Format

### HDF5 Map Files

The semantic maps are saved in HDF5 format with the following structure:

```
map_file.h5
├── full_map              # Shape: (num_channels, height, width)
│   ├── [0] obstacle_map      # Obstacle predictions
│   ├── [1] explored_map      # Explored areas
│   ├── [2] agent_map         # Current agent location
│   ├── [3] past_locations_map # Agent trajectory
│   └── [4:] semantic_maps    # Semantic category predictions
├── semantic_map          # Shape: (num_categories, height, width)
├── obstacle_map          # Shape: (height, width)
├── explored_map          # Shape: (height, width)
├── agent_map             # Shape: (height, width)
├── past_locations_map    # Shape: (height, width)
└── metadata/             # Experiment parameters and pose information
    ├── step              # Simulation step number
    ├── pose_x            # Agent X position (meters)
    ├── pose_y            # Agent Y position (meters)
    ├── pose_theta        # Agent orientation (radians)
    ├── map_size_cm       # Map size in centimeters
    ├── map_resolution    # Map resolution (cm per pixel)
    └── num_categories    # Number of semantic categories
```

### Map Coordinates

- **Map Size**: 2400cm × 2400cm (24m × 24m) by default
- **Resolution**: 5cm per pixel by default
- **Map Shape**: (480, 480) pixels
- **Coordinate System**: 
  - Origin at top-left corner
  - X-axis points right (East)
  - Y-axis points down (South)
  - Angles measured clockwise from East

## Technical Details

### Neural Semantic Mapping Pipeline

1. **Input**: RGB-D observation + pose change
   - RGB: 3 channels (640×480)
   - Depth: 1 channel (640×480)
   - Semantics: 21 channels (640×480) - semantic predictions from RGB
   - Pose: [dx, dy, dtheta] - relative movement

2. **Processing**: Neural network transforms observations into map updates
   - Point cloud generation from depth
   - 3D to 2D projection onto map grid
   - Semantic feature aggregation
   - Pose estimation and map registration

3. **Output**: Updated semantic map
   - Obstacle predictions
   - Explored area estimates
   - Semantic category predictions per pixel
   - Agent location and trajectory

### Dependencies

The scripts require the following PONI modules:
- `semexp.model.Semantic_Mapping`: Core neural mapping module
- `hlab.utils.semantic_mapping.Semantic_Mapping`: Configuration wrapper

Make sure you have PONI properly installed with all dependencies.

## Example Workflow

1. **Generate predicted maps**:
   ```bash
   python create_predicted_semantic_maps.py --num_steps 15 --visualize --save_dir ./my_experiment
   ```

2. **Visualize results**:
   ```bash
   python visualize_predicted_maps.py --map_dir ./my_experiment --create_animation --save_dir ./my_visualizations
   ```

3. **Analyze specific map**:
   ```bash
   python visualize_predicted_maps.py --map_file ./my_experiment/final_semantic_map.h5
   ```

This will generate a complete set of predicted semantic maps showing how PONI's neural network incrementally builds semantic understanding during navigation.

## Comparison with Ground Truth

To compare neural predictions with ground truth semantic maps:

1. Use the neural scripts above to generate predicted maps
2. Use `create_semantic_maps.py` to generate ground truth maps from 3D scenes
3. Compare the outputs to understand the difference between:
   - **Ground Truth**: Complete, accurate semantic maps from full 3D scenes
   - **Neural Predictions**: Partial, estimated semantic maps from limited RGB-D observations

The neural approach simulates realistic navigation conditions where an agent must build semantic understanding incrementally from partial observations, which is the core challenge that PONI addresses.
